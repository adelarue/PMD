{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method_category (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function method_category(met)\n",
    "    if startswith(meth, \"Imp-then-Reg\")\n",
    "        return \"Imp-then-Reg\"\n",
    "    elseif startswith(meth, \"Joint Imp-then-Reg\")\n",
    "        return \"Joint Imp-then-Reg\"\n",
    "    elseif meth ∈ [\"Static\", \"Affine\", \"Finite\"]\n",
    "        return \"Adaptive LR\"\n",
    "    elseif startswith(meth, \"Complete Features\")\n",
    "        return \"Complete Features\"\n",
    "    else \n",
    "        return meth\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{String}:\n",
       " \"cylinder-bands\"\n",
       " \"ozone-level-detection-eight\"\n",
       " \"ozone-level-detection-one\"\n",
       " \"thyroid-disease-thyroid-0387\"\n",
       " \"trains\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb_datasets = [\"cylinder-bands\", \"ozone-level-detection-eight\", \"ozone-level-detection-one\", \"thyroid-disease-thyroid-0387\", \"trains\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Real X - Syn Y Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setting = \"fakey/\"\n",
    "for dir = [\"tree_mar/\", \"tree_nmar/\", \"tree_mar_adv/\"]\n",
    "    directory = setting*dir\n",
    "    subdir = \"final/\"\n",
    "    \n",
    "    filelist = [f for f in readdir(directory*subdir) if endswith(f, \".csv\")]\n",
    "    res = similar(CSV.read(directory*subdir*filelist[1], DataFrame),0)\n",
    "    for i in 1:length(filelist)\n",
    "        res = vcat(res, CSV.read(directory*subdir*filelist[i], DataFrame))\n",
    "    end\n",
    "\n",
    "    res[!,:method_cat] = map(t -> method_category, res[:,:method])\n",
    "    res[!,:X_setting] .= \"real_X_\"*split(dir, \"_\")[2][1:end-1]\n",
    "    res[!,:Y_setting] .= \"syn_Y_\"*split(dir, \"_\")[1]\n",
    "\n",
    "    CSV.write(directory*\"FINAL_results.csv\", res)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = vcat(CSV.read(setting*\"tree_mar/\"*\"FINAL_results.csv\", DataFrame),\n",
    "    CSV.read(setting*\"tree_nmar/\"*\"FINAL_results.csv\", DataFrame),\n",
    "    CSV.read(setting*\"tree_mar_adv/\"*\"FINAL_results.csv\", DataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter!(t -> t[:dataset] ∉ pb_datasets, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for method in [\"Complete Features\", \"Imp-then-Reg 1\", \"Imp-then-Reg 2\", \"Imp-then-Reg 3\", \"Imp-then-Reg 4\", \"Joint Imp-then-Reg\"]\n",
    "    aux = filter(t -> startswith(t[:method], method), res)\n",
    "    # @show size(aux)\n",
    "    idcols = [:dataset, :X_setting, :Y_setting, :SNR, :k, :kMissing, :splitnum]\n",
    "    gd = groupby(aux, idcols)\n",
    "\n",
    "    aux = similar(aux, 0)\n",
    "    for subdf in gd \n",
    "        scoremax = argmax(subdf[:,:score])\n",
    "        push!(aux, subdf[scoremax,names(aux)])\n",
    "    end\n",
    "    aux[!,:method] .= method*\" - best\"\n",
    "\n",
    "    res = vcat(res, aux)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSV.write(setting*\"tree_mar/\"*\"FINAL_TREE_results.csv\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = vcat(CSV.read(setting*\"tree_mar/\"*\"FINAL_results.csv\", DataFrame),\n",
    "    CSV.read(setting*\"tree_nmar/\"*\"FINAL_results.csv\", DataFrame),\n",
    "    CSV.read(setting*\"tree_mar_adv/\"*\"FINAL_results.csv\", DataFrame))\n",
    "\n",
    "filter!(t -> t[:dataset] ∉ pb_datasets, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gd = groupby(unique(res[:,[:dataset, :X_setting, :kMissing, :splitnum]]), \n",
    "    [:dataset, :X_setting, :kMissing])\n",
    "unique(\n",
    "    innerjoin(res, \n",
    "        filter(t -> t[:nrow] < 10, combine(gd, nrow)), on = [:dataset, :X_setting, :kMissing])[:,[:dataset, :X_setting, :kMissing]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check = leftjoin(combine(groupby(unique(res[:,[:dataset,:X_setting, :kMissing]]), [:dataset,:X_setting]), nrow), \n",
    "    df, \n",
    "    on =:dataset)\n",
    "check[(check[:,:nrow] .< check[:,:p_miss_num] .+ 1),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_list = [d for d in readdir(\"../datasets/\") if !startswith(d, \".\")]\n",
    "sort!(dataset_list)\n",
    "redolist = []            \n",
    "for d in unique(check[(check[:,:nrow] .< check[:,:p_miss_num] .+ 1),:dataset]) # unique(filter(t -> t[:nrow] < 10, combine(gd, nrow))[:,:dataset])\n",
    "    @show d, findfirst(dataset_list .== d) - 1\n",
    "                push!(redolist, findfirst(dataset_list .== d) - 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "4,5,10,21,23,24,31,38,43,49,61,62,63,64,65,66,67,68,70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = CSV.read(\"pattern_counts_allfeat.csv\", DataFrame) #Before one-hot-encoding\n",
    "select!(df1, [:Name, :n, :p, :Num_Patterns, :p_miss])\n",
    "rename!(df1, :p => :p_all, :p_miss => :p_miss_all)\n",
    "df2 = CSV.read(\"pattern_counts_numonly.csv\", DataFrame) #After one-hot-encoding\n",
    "select!(df2, [:Name, :p, :p_miss])\n",
    "rename!(df2, :p => :p_num, :p_miss => :p_miss_num)\n",
    "df = innerjoin(df1, df2, on=:Name, makeunique=true)\n",
    "df[!,:missing_num] .= df[:,:p_miss_num]\n",
    "df[!,:missing_cat] .= df[:,:p_miss_all] .- df[:,:p_miss_num]\n",
    "rename!(df, :Name => :dataset)\n",
    "select!(df, [:dataset, :missing_cat, :p_miss_all, :p_miss_num, :p_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Real Data Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for directory = [\"realy/\"]\n",
    "    subdir=\"2022-08-23/\"\n",
    "    filelist = [f for f in readdir(directory*subdir) if endswith(f, \".csv\") && f ∉ [\"all_results.csv\",\"all_results_new.csv\"]]\n",
    "    res = similar(CSV.read(directory*subdir*filelist[1], DataFrame),0)\n",
    "    for i in 1:length(filelist)\n",
    "        res = vcat(res, CSV.read(directory*subdir*filelist[i], DataFrame))\n",
    "    end\n",
    "    # filter!(t -> t[:k] > 0, res) #Remove dataset with only a bias term\n",
    "    res[!,:method_cat] = map(t -> method_category, res[:,:method])\n",
    "    res[!,:X_setting] .= \"real_X\"\n",
    "    res[!,:Y_setting] .= \"real_Y\"\n",
    "    CSV.write(directory*\"FINAL_results.csv\", res)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = CSV.read(\"realy/\"*\"FINAL_results.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique(filter( t-> t[:nrow] < 10, combine(groupby(res, [:dataset, :method]), nrow))[:,[:dataset, :nrow]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = CSV.read(\"realy/\"*\"FINAL_results.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter!(t -> t[:dataset] ∉ pb_datasets, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for method in [\"Complete Features\", \"Imp-then-Reg 1\", \"Imp-then-Reg 2\", \"Imp-then-Reg 3\", \"Imp-then-Reg 4\", \"Joint Imp-then-Reg\"]\n",
    "    aux = filter(t -> startswith(t[:method], method), res)\n",
    "    # @show size(aux)\n",
    "    idcols = [:dataset, :SNR, :k, :kMissing, :splitnum]\n",
    "    gd = groupby(aux, idcols)\n",
    "\n",
    "    aux = similar(aux, 0)\n",
    "    for subdf in gd \n",
    "        scoremax = argmax(subdf[:,:score])\n",
    "        push!(aux, subdf[scoremax,names(aux)])\n",
    "    end\n",
    "    aux[!,:method] .= method*\" - best\"\n",
    "\n",
    "    res = vcat(res, aux)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSV.write(\"realy/\"*\"FINAL_results.csv\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Synthetic-Data Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dir = [\"linear_mar/\", \"linear_censoring/\", \"nn_mar/\", \"nn_censoring/\", \"tree_mar/\", \"tree_censoring/\"]\n",
    "    directory = \"synthetic/\"*dir\n",
    "    subdir=\"final/\"\n",
    "    \n",
    "    filelist = [f for f in readdir(directory*subdir) if endswith(f, \".csv\") && f ∉ [\"all_results.csv\",\"all_results_new.csv\"]]\n",
    "    res = similar(CSV.read(directory*subdir*filelist[1], DataFrame),0)\n",
    "    for i in 1:length(filelist)\n",
    "        aux = CSV.read(directory*subdir*filelist[i], DataFrame)\n",
    "        if any(aux[:,:pMissing] .> 0)\n",
    "            missingproba = unique(aux[aux[:,:pMissing] .> 0,:pMissing])[1]\n",
    "            aux[!,:pMissing] .= missingproba\n",
    "        end\n",
    "        try\n",
    "            res = vcat(res, aux)\n",
    "        catch \n",
    "            println(\"Error with \", directory*subdir*filelist[i])\n",
    "        end\n",
    "    end\n",
    "    # filter!(t -> t[:k] > 0, res) #Remove dataset with only a bias term\n",
    "    res[!,:method_cat] = map(t -> method_category, res[:,:method])\n",
    "    res[!,:X_setting] .= \"syn_X_\"*split(dir, \"_\")[2][1:end-1]\n",
    "    res[!,:Y_setting] .= \"syn_Y_\"*split(dir, \"_\")[1]\n",
    "\n",
    "    CSV.write(directory*\"FINAL_results.csv\", res)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = CSV.read(\"synthetic/linear_mar/FINAL_results.csv\", DataFrame) \n",
    "df[!,:setting] .= \"1 - Lin-MAR\"\n",
    "\n",
    "aux = CSV.read(\"synthetic/linear_censoring/FINAL_results.csv\", DataFrame) \n",
    "aux[!,:setting] .= \"2 - Lin-NMAR\"\n",
    "df = vcat(df, aux)\n",
    " \n",
    "aux = CSV.read(\"synthetic/tree_mar/FINAL_results.csv\", DataFrame) \n",
    "aux[!,:setting] .= \"3 - Tree-MAR\"\n",
    "df = vcat(df, aux)\n",
    "\n",
    "aux = CSV.read(\"synthetic/tree_censoring/FINAL_results.csv\", DataFrame) \n",
    "aux[!,:setting] .= \"4 - Tree-NMAR\"\n",
    "df = vcat(df, aux)\n",
    "\n",
    "aux = CSV.read(\"synthetic/nn_mar/FINAL_results.csv\", DataFrame) \n",
    "aux[!,:setting] .= \"5 - NN-MAR\"\n",
    "df = vcat(df, aux)\n",
    "\n",
    "aux = CSV.read(\"synthetic/nn_censoring/FINAL_results.csv\", DataFrame) \n",
    "aux[!,:setting] .= \"6 - NN-NMAR\"\n",
    "df = vcat(df, aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine(groupby(df, [:dataset, :method, :setting]), nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique(combine(groupby(df, [:dataset, :method, :setting]), nrow)[:,:nrow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter( t-> t[:nrow] < 90, combine(groupby(res, [:dataset, :method, :setting]), nrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbdata = unique(filter( t -> t[:nrow] < 90, combine(groupby(df, [:dataset, :method, :setting]), nrow))[:,[:method, :setting, :dataset]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aux = filter( t -> t[:dataset] ∈ pbdata && t[:nrow] < 10, combine(groupby(df, [:dataset, :method, :setting, :pMissing]), nrow))\n",
    "unique(aux[:,[:dataset, :setting, :pMissing, :nrow]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter(t -> t[:pMissing] == 0.1, unique(aux[:,[:dataset, :setting, :pMissing, :nrow]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter(t -> t[:pMissing] == 0.3, unique(aux[:,[:dataset, :setting, :pMissing, :nrow]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter(t -> startswith(t[:setting], \"5\"), unique(aux[:,[:dataset, :setting, :pMissing, :nrow]]))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
